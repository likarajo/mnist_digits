# -*- coding: utf-8 -*-
"""nn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NvzRBE6v1BH_JF0BWFA8YKLb7urLrmWf
"""

################################################################################
#
# LOGISTICS
#
#    Rajarshi Chattopadhyay
#    RXC170010
#
# DESCRIPTION
#
#    MNIST image classification with an NN written and trained in Python
#
# INSTRUCTIONS
#
#    1. Go to Google Colaboratory: https://colab.research.google.com/notebooks/welcome.ipynb
#    2. File - New Python 3 notebook
#    3. Cut and paste this file into the cell (feel free to divide into multiple cells)
#    4. Runtime - Run all
#
# NOTES
#
#    1. This does not use PyTorch, TensorFlow or any other xNN library
#
#    2. Summary of nn.py
#
# The neural network has 4 layers in total: 1 input layer, 2 hidden layers and 1 output layer. 
# All layers are fully connected.
# Input layer: Input consisting of 28x28 images. We flatten these images to have 28x28=784 nodes.
# Hidden layer 1: 1000 nodes. 
# Hidden layer 2: 100 nodes
# Output layer: Reducing to a total of 10 nodes, so that we can evaluate the nodes against the label. 
# This label is received in the form of an array with 10 elements, where one of the elements is 1, while the rest is 0.
#
# Plot Training loss per epoch
# Plot Test accuracy per epoch
# Plot Training time per epoch
#
#
################################################################################

################################################################################
#
# IMPORT
#
################################################################################

#
# you should not need any import beyond the below
# PyTorch, TensorFlow, ... is not allowed
#

import os.path
import urllib.request
import gzip
import math
import numpy             as np
import matplotlib.pyplot as plt
import time

################################################################################
#
# PARAMETERS
#
################################################################################

# data
DATA_NUM_TRAIN = 60000
DATA_NUM_TEST = 10000
DATA_CHANNELS = 1
DATA_ROWS = 28
DATA_COLS = 28
DATA_CLASSES = 10
DATA_URL_TRAIN_DATA = 'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz'
DATA_URL_TRAIN_LABELS = 'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz'
DATA_URL_TEST_DATA = 'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz'
DATA_URL_TEST_LABELS = 'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'
DATA_FILE_TRAIN_DATA = 'train_data.gz'
DATA_FILE_TRAIN_LABELS = 'train_labels.gz'
DATA_FILE_TEST_DATA = 'test_data.gz'
DATA_FILE_TEST_LABELS = 'test_labels.gz'

# display
DISPLAY_ROWS = 8
DISPLAY_COLS = 4
DISPLAY_COL_IN = 10
DISPLAY_ROW_IN = 25
DISPLAY_NUM = DISPLAY_ROWS * DISPLAY_COLS

################################################################################
#
# DATA
#
################################################################################

# download
if not os.path.exists(DATA_FILE_TRAIN_DATA):
    urllib.request.urlretrieve(DATA_URL_TRAIN_DATA, DATA_FILE_TRAIN_DATA)
if not os.path.exists(DATA_FILE_TRAIN_LABELS):
    urllib.request.urlretrieve(DATA_URL_TRAIN_LABELS, DATA_FILE_TRAIN_LABELS)
if not os.path.exists(DATA_FILE_TEST_DATA):
    urllib.request.urlretrieve(DATA_URL_TEST_DATA, DATA_FILE_TEST_DATA)
if not os.path.exists(DATA_FILE_TEST_LABELS):
    urllib.request.urlretrieve(DATA_URL_TEST_LABELS, DATA_FILE_TEST_LABELS)

# training data
# unzip the file, skip the header, read the rest into a buffer and format to NCHW
file_train_data = gzip.open(DATA_FILE_TRAIN_DATA, 'r')
file_train_data.read(16)
buffer_train_data = file_train_data.read(DATA_NUM_TRAIN * DATA_ROWS * DATA_COLS)
train_data = np.frombuffer(buffer_train_data, dtype=np.uint8).astype(np.float32)
train_data = train_data.reshape(DATA_NUM_TRAIN, 1, DATA_ROWS, DATA_COLS)

# training labels
# unzip the file, skip the header, read the rest into a buffer and format to a vector
file_train_labels = gzip.open(DATA_FILE_TRAIN_LABELS, 'r')
file_train_labels.read(8)
buffer_train_labels = file_train_labels.read(DATA_NUM_TRAIN)
train_labels = np.frombuffer(buffer_train_labels, dtype=np.uint8).astype(np.int32)

# testing data
# unzip the file, skip the header, read the rest into a buffer and format to NCHW
file_test_data = gzip.open(DATA_FILE_TEST_DATA, 'r')
file_test_data.read(16)
buffer_test_data = file_test_data.read(DATA_NUM_TEST * DATA_ROWS * DATA_COLS)
test_data = np.frombuffer(buffer_test_data, dtype=np.uint8).astype(np.float32)
test_data = test_data.reshape(DATA_NUM_TEST, 1, DATA_ROWS, DATA_COLS)

# testing labels
# unzip the file, skip the header, read the rest into a buffer and format to a vector
file_test_labels = gzip.open(DATA_FILE_TEST_LABELS, 'r')
file_test_labels.read(8)
buffer_test_labels = file_test_labels.read(DATA_NUM_TEST)
test_labels = np.frombuffer(buffer_test_labels, dtype=np.uint8).astype(np.int32)

# debug
# print("Train data", train_data.shape)   # (60000, 1, 28, 28)
# print("Train labels", train_labels.shape) # (60000,)
# print("Test data", test_data.shape)    # (10000, 1, 28, 28)
# print("Test label", test_labels.shape)  # (10000,)


################################################################################
#
# YOUR CODE GOES HERE
#
################################################################################

# Functions

# Activation function
def relu(a, derivative=False):
  if derivative:
    return np.greater(a,0).astype(int)
  else:
    return np.maximum(0,a)

def softmax(X):
  exps = np.exp(X - np.max(X))
  return exps / np.sum(exps)

# Loss function
def cross_entropy(X,y):
    """
    X : (num_examples x num_classes)
    y : (num_examples x 1) computed as y.argmax(axis=1) from one-hot encoded vectors of labels
    """
    m = y.shape[0]
    p = softmax(X)
    log_likelihood = -np.log(p[range(m),y])
    loss = np.sum(log_likelihood) / m
    return loss
  
def delta_cross_entropy(X,y):
    """
    X : (num_examples x num_classes)
    y : (num_examples x 1) computed as y.argmax(axis=1) from one-hot encoded vectors of labels
    """
    m = y.shape[0]
    grad = softmax(X)
    grad[range(m),y] -= 1
    grad = grad/m
    return grad

# Config and hyper parameters

LAYERS_CONFIG = [784, 1000, 100, 10]
LEARNING_RATE = 0.001
NUM_EPOCHS = 5

# Modeling

# one hot encoding labels
one_hot_labels = np.zeros((train_labels.shape[0], 10))
for i in range(train_labels.shape[0]):
    one_hot_labels[i, train_labels[i]] = 1
one_hot_labels = np.array(one_hot_labels, dtype='float32')
print("Train Labels:", one_hot_labels.shape)

# normalize data
train_data = (train_data/255).astype('float32')
test_data = (test_data/255).astype('float32')

# vectorize data
train_data = train_data.reshape(train_data.shape[0],1,28*28)
test_data = test_data.reshape(test_data.shape[0],1,28*28)
print("Train Data:", train_data.shape)
print("Test Data:", test_data.shape)

# initialize weights
w1 = np.random.randn(train_data.shape[2], LAYERS_CONFIG[1])*np.sqrt(1./train_data.shape[2])
b1 = np.random.randn(LAYERS_CONFIG[1])
w2 = np.random.randn(w1.shape[1], LAYERS_CONFIG[2])*np.sqrt(1./w1.shape[1])
b2 = np.random.randn(LAYERS_CONFIG[2])
w3 = np.random.randn(w2.shape[1], LAYERS_CONFIG[3])*np.sqrt(1./w2.shape[1])
b3 = np.random.randn(LAYERS_CONFIG[3])

print("Layers -")
print('w1:', w1.shape, 'b1:', b1.shape)
print('w2:', w2.shape, 'b2:', b2.shape)
print('w3:', w3.shape, 'b3:', b3.shape)


# Training
print("Training started...")

train_loss_list = []
test_acc_list = []
train_time_list = []

start_time = time.time()

# Iterations
for epoch in range(NUM_EPOCHS):
  s = time.time()

  loss=0
  for i,val in enumerate(train_data):

    # forward pass

    # input layer
    temp_input = np.copy(val)
    # hidden layer 1
    x1 = np.dot(temp_input, w1)
    z1 = np.add(x1,b1)
    a1 = relu(z1)
    # hidden layer 2
    x2 = np.dot(a1, w2)
    z2 = np.add(x2,b2)
    a2 = relu(z2)
    # output layer
    x3 = np.dot(a2, w3)
    z3 = np.add(x3,b3)
    output = softmax(z3)

    # back-propagation

    # output layer loss and derivatives
    loss_z3 = output - one_hot_labels[i]
    z3_w3 = a2
    d_w3 = np.dot(z3_w3.T, loss_z3)
    d_b3 = loss_z3
    # hidden layer 2 derivatives
    z3_a2 = w3
    loss_a2 = np.dot(loss_z3 , z3_a2.T)
    a2_z2 = relu(z2, derivative=True)
    z2_w2 = a1
    d_w2 = np.dot(z2_w2.T, a2_z2 * loss_a2)
    d_b2 = loss_a2 * a2_z2
    # hidden layer 1 derivatives
    loss_z2 = loss_a2*a2_z2
    z2_a1 = w2
    loss_a1 = np.dot(loss_z2 , z2_a1.T)
    da1_dz1 = relu(z1, derivative=True)
    z1_w1 = temp_input
    d_w1 = np.dot(z1_w1.T, da1_dz1 * loss_a1)   
    d_b1 = loss_a1 * da1_dz1

    # weights updates
    lr = LEARNING_RATE
    w1 -= lr * d_w1
    b1 -= lr * d_b1.sum(axis=0)
    w2 -= lr * d_w2
    b2 -= lr * d_b2.sum(axis=0)
    w3 -= lr * d_w3
    b3 -= lr * d_b3.sum(axis=0)

    loss = np.sum(-one_hot_labels[i] * np.log(output))
  
  train_loss_list.append(loss)

  correct = 0
  for i, val in enumerate(test_data):

    # forward pass

    # input layer
    temp_input = np.copy(val)
    # hidden layer 1
    x1 = np.dot(temp_input, w1)
    z1 = np.add(x1, b1)
    a1 = relu(z1)
    # hidden layer 2
    x2 = np.dot(a1, w2)
    z2 = np.add(x2, b2)
    a2 = relu(z2)
    # output layer
    x3 = np.dot(a2, w3)
    z3 = np.add(x3, b3)
    output = softmax(z3)

    if(np.argmax(output) == test_labels[i]):
      correct += 1

  acc = correct/len(test_data)
  test_acc_list.append(acc)

  t_time = time.time()-s
  train_time_list.append(t_time)

  print('Epoch: {} Time: {:.2f}s Train Loss: {:.4f} Test Acc: {:.4f}'.format(epoch+1, t_time, loss, acc*100))

time_taken = time.time()-start_time


# Testing (with w and b)

predicted_label_list = []
correct = 0
for i, val in enumerate(test_data):

  # forward pass

  # input layer
  temp_input = np.copy(val)
  # hidden layer 1
  x1 = np.dot(temp_input, w1)
  z1 = np.add(x1, b1)
  a1 = relu(z1)
  # hidden layer 2
  x2 = np.dot(a1, w2)
  z2 = np.add(x2, b2)
  a2 = relu(z2)
  # output layer
  x3 = np.dot(a2, w3)
  z3 = np.add(x3, b3)
  output = softmax(z3)
  predicted_label_list.append(np.argmax(output))

  if(np.argmax(output) == test_labels[i]):
    correct += 1

acc = correct/len(test_data)
print("Total accuracy is {0:.4f}%".format(acc*100))

# plot of train loss per epoch
plt.plot(list(range(0,NUM_EPOCHS)), train_loss_list)
plt.xlabel('EPOCH')
plt.ylabel('Training Cross Entropy Loss')
plt.show()

# plot of test accuracy per epoch
plt.plot(list(range(0,NUM_EPOCHS)), test_acc_list)
plt.xlabel('EPOCH')
plt.ylabel('Test Accuracy')
plt.show()

# performance display
# total time
print('Training Time Taken: ', time_taken)

# plot of train time per epoch
plt.plot(list(range(0,NUM_EPOCHS)), train_time_list)
plt.xlabel('EPOCH')
plt.ylabel('Training Time')
plt.show()

# example display
# replace the xNN predicted label with the label predicted by the network
fig = plt.figure(figsize=(DISPLAY_COL_IN, DISPLAY_ROW_IN))
ax  = []
for i in range(DISPLAY_NUM):
    img = test_data[i, :, :].reshape((DATA_ROWS, DATA_COLS))
    ax.append(fig.add_subplot(DISPLAY_ROWS, DISPLAY_COLS, i + 1))
    ax[-1].set_title('True: ' + str(test_labels[i]) + ' xNN: ' + str(predicted_label_list[i]))
    plt.imshow(img, cmap='Greys')
plt.show()